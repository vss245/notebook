# Introduction to statistical learning

## Chapter 1: Intro

- Statistical learning - tools for understanding data
  - Supervised - estimating output based on inputs
  - Unsupervised - inputs and no supervising output
- Types of problems:
  - Regression (continuous or quantitative output): the relationship between wage and age, education, calendar year 
  - Classification (discrete output): predicting whether stock price will go up or down on a given day using previous day's change
  - Clustering (unsupervised): grouping customers along some dimensions to fit recommendations better
- Brief history:
  - 1800s - Gauss and Legendre - method of least squares (earliest linear regression)
  - 1936 - Fisher - linear discriminant analysis
  - 1940s - logistic regression
  - 1970s - generalized linear models (class of models, linear and logistic regression are special cases)
  - 1980s - classification and regression trees
  - 1986 - Hastie and Tibshirani - generalized additive models (non-linear GLM extension)
  - (among many other methods)
- Matrix notation:
  - $\bold{X}$ is an n x p matrix with $x_{ij}$ at i, j
  - n samples, p variables
  - $\bold{X}^T$ is the transpose of a matrix

## Chapter 2: Statistical learning

- Applying statistics to advertising: predict how advertising budget impacts sales

- Input - budgets (independent variable, X), output - sales (dependent variable, Y)

- We assume that there is a relationship $Y = f(X)+\epsilon$, where f is the function of X and epsilon is the error term (noise)
- We want to estimate f for prediction ($\hat{Y}=\hat{f}(X)$) for some Xs
  - The accuracy will depend on the reducible error (minimized by picking the right f) and the irreducible error (inherent, affected by noise)
  - $E(Y-\hat{Y})^2 = E[f(X)+\epsilon - \hat{f}(X)]^2 = [f(X)- \hat{f}(X)]^2 + Var(\epsilon)$
  - E is the average (expected) value of the squared residuals and Var(e) is the variance associated with the error term
- We might also be interested in inference - understanding $f$
  - What predictors are important, what is the relationship, is it linear vs non-linear
- Methods of estimating $f$
  - We can split the data into the training and testing dataset and apply a statistical method to it to find a function $\hat{f}$ such that $Y \approx \hat{f}(X)$ for (X,Y)
  - Parametric methods:
    - Require an assumption about the shape of f, for example, that f is linear: $f(X)= \beta_0 + \beta_1X_1 + \beta_2X_2 ...$
    - In this case, we will be estimating only the p+1 parameters
    - Select a procedure for estimating the coefficients such as least squares
    - Advantages: simple estimation
    - Disadvantages: model might be too simple or too complex and overfit the data
  - Non-parametric methods:
    - No assumptions about the shape of f, only looking to get as close to the datapoints as possible without overfitting
    - Need a large number of observations to estimate the parameters
    - Advantages: no assumptions
    - Disadvantages: need a lot of data, could overfit, less interpretable
- There is a trade-off between accuracy and interpretability
  - Simple models like linear regression might be less accurate, but easier to interpret
  - GAMs allow non-linear relationships for better fitting, but are harder to interpret
  - Some models are very flexible but extremely hard to interpret, such as non-linear kernel SVMs
- Variables can be quantitative (numerical) or qualitative (categories, labels), and are most commonly seen in regression or classification problems, respectively
- Model accuracy - how do we know if the fit is good?
  - No free lunch - there is no single best algorithm
  - Quality of fit
    - **Mean square error = $\frac{1}{n}\sum_{i=1}^n{(y_i-\hat{f}(x_i))^2}$**
    - We are interested in MSE when f(x) is applied to testing data, and not just training
    -  Very small MSE on training data may mean overfitting
    - MSE often is a U-shaped function of complexity, models that are too simple and too complex will not have a good MSE
  - Bias-variance trade-off
    - Variance - how much $\hat{f}$ would change if estimated on a different training data set
      - More flexible methods have higher variance
    - Bias - how far from the true data is the model
      - More flexible methods have less bias
  - Accuracy for classification
    - **Error rate = $\frac{1}{n}\sum_{i=1}^n{I(y_i \neq \hat{y_i})}$**
    - I is the indicator variable (1 if predicted not equal to true)
- Classification methods
  - Bayes classifier
    - Test error rate is minimized by a classifier that assigns each observation to the most likely class given the predictors
      - $Pr(Y=j|X=x_0)$
      - Conditional probability - P of Y = j if X = x0
    - Results in a Bayes decision boundary
  - K-nearest neighbors
    - Classifies based on estimated probability (most neighbors out of K belonging to a class)
    - $Pr(Y = j|X=x_0)=\frac{1}{K} \sum_{i \in \Nu_0}I(y_i=j)$
    - Lower K will produce a more flexible boundary

## Chapter 3: Linear regression

- **Simple linear regression: $Y \approx \beta_0+\beta_1X$**
- Need to estimate the parameters $\beta$ and produce $\hat{y}=\hat{\beta_0}+\hat{\beta_1}x$
- Common approach: minimizing the least squares criterion
- **RSS (residual sum of squares) = $e_1^2 + e_2^2 ...$, where $e_i=y_i-\hat{y_i}$**
- Minimizers:
  - $\hat{\beta_1}=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}$
  - $\hat{\beta_0}=\bar{y}-\hat{\beta_1}\bar{x}$
- Accuracy measures for coefficients:
  - The population regression line (the true relationship) is usually unobserved (we only have a finite sample that we can estimate the relationship from)
  - Bias: an unbiased estimator does not over- or under-estiamte the true parameter (e.g. when estimating the population mean from a sample mean), given a large enough sample size
  - If we try to estimate the population mean $\mu$ of a variable Y, how accurate is the sample mean estimation $\hat{\mu}$? The average of estimates over many data sets will be close, but for one sample, it could be way off
    - **Standard error: $Var(\hat{\mu})=SE(\hat{\mu})^2=\frac{\sigma^2}{n}$**, where sigma is the standard deviation (this tells us the average amount that our estimate differs from the actual value)
    - We can also compute the standard errors associated with $\hat{\beta}$, e.g.: $SE(\hat{\beta}_1^2) = \frac{\sigma^2}{\sum_{i=1}^n(x_i-\bar{x})^2}$
    - In general, we don't know $\sigma^2$, but we can estimate it from the data using the **residual standard error** ($\sqrt{RSS/(n-2)}$)
    - Standard errors can be used to compute **confidence intervals**:
      - $\hat{\beta_1} Â± 2*SE(\hat{\beta_1})$ (~95%)
- Hypotheses (null and alternative):
  - $H_0$: there is no relationship between X and Y ($\beta_1=0$)
  - $H_a$: there is some relationship between X and Y ($\beta_1\neq0$)
- Hypothesis testing:
  - If $\beta_1=0$, then the model is just $Y = \beta_0+\epsilon$ and X has no association with Y
  - We compute a **t-statistic**, which measures how many standard deviations $\hat{\beta}_1$ is from 0
  - $t = \frac{\hat{\beta}_1-0}{SE(\hat{\beta}_1)}$
  - The t-distribution has a bell shape and approaches the normal distribution for n > 30
  - From there, we can compute the probability of observing a number equal to |t| if $\beta_1=0$ (if the null hypothesis is correct) - this is the **p-value**
  - If the p-value is small enough, we conclude that we can reject the null hypothesis
- Accuracy measures for the model:
  - Quantify the extent to which the model fits the data
  - **Residual standard error (RSE)**
    - Estimate of the standard deviation of $\epsilon$ (average amount that the response will deviate from the true line)
    - $RSE = \sqrt{\frac{1}{n-2}RSS} = \sqrt{\frac{1}{n-2}\sum_{i=1}^n(y_i-\hat{y_i})^2}$
    - Measure of the lack of fit
  - **$R^2$ statistic**
    - Alternative measure of fit - proportion of variance explained (between 0 and 1)
    - $R^2 = 1 - \frac{RSS}{TSS} = 1 - \frac{\sum_{i=1}^n(y_i-\hat{y_i})^2}{\sum(y_i-\bar{y})^2}$
    - **R-squared is the fraction between the residual sum of squares** (sum of squared differences between true and predicted value) **and the total sum of squares**(total variance in response e.g. total amount of variability inherent in the data)
    - It represents the proportion of variability in Y that can be explained using X
  - **Correlation**
    - $r = Cor(X,Y)=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^n(x_i-\bar{x})^2}\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}}$
    - Measures the linear relationship between X and Y and for simple linear regression, $R^2 = r^2$
    - But correlation quantifies the association between a single pair of variables and not a large number of variables, so correlation is not usable for cases like multiple linear regression
- Multiple linear regression
  - If we have more than one predictor, we can fit a separate model for each, but this complicates prediction and doesn't account for correlation between the predictors
  - We can give each predictor a separate slope coefficient
    - $Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p + \epsilon$
  - We can estimate the coefficients using the same approach as simple linear regression (e.g. sum of squared residuals)
  - The regression line will become a plane/hyperplane with >1 predictor
  - The results (coefficients and p-values) of predictors in multiple linear regression can be very different from simple linear regression, particularly if there is a correlation between predictors
- Questions to ask
  - **Is at least one of the predictors $X_1, X_2, ..., X_p$ useful in predicting the response?**
    -  We can use a hypothesis test to answer this question
    - $H_0: \beta_1 = \beta_2 = ... = \beta_p = 0$
    - $H_a:$ at least one $\beta_j$ is non-zero
    - We can compute the F-statistic: $F = \frac{(TSS-RSS)/p}{RSS/(n-p-1)}$, where $TSS = \sum(y_i-\bar{y})^2$ (sum of differences between each point and mean) and $RSS = \sum(y_i-\hat{y}_i)^2$ (sum of differences between each point and prediction)
    - If there is no relationship between the response and predictors, F-statistic will be close to 1
    - How do we know which value is good? If n is large, then even a small difference from 1 will be good evidence, but we will need a large F-statistic for a small sample size
    - We can also test if a particular subset of coefficients are zero:
      - Fit a reduced model omitting those variables, calculate the RSS ($RSS_0$)
      - $F = \frac{(RSS_0 - RSS)/q}{RSS/(n-p-1)}$
    - Individual t-statistics and p-values are equivalent to the results of omitting those from the model (e.g. the partial effect of adding that variable to the model)
    - Why do we need this? For very large numbers of predictors, we might see very small p-values even in the absence of any true association! But the F-statistic will adjust for the number of predictors.
    - In the case where p > n, there are more coefficients to estimate than observations from which to estimate them. In this case, we can't even fit the model using least squares and will need other approaches
  - **Do all predictors help explain Y, or is only a subset useful?**
    - If we compute the F-statistic and find that at least one of the predictors is useful, we need to find out which.
    - Variable selection - finding which predictors are associated with the response
    - We can try models using the full set of predictors or subsets and use a statistic to judge the quality of a model
      - Methods: Akaike's information criterion, Bayesian information criterion, adjusted $R^2$
    - If p is large, this is intractable and we need other methods
      - Forward selection: begin with the null model (intercept only), fit p regressions and add the variables with the lowest RSS (greedy approach - can include predictors that become useless)
      - Backward selection: start with all variables and remove the variable with the largest p-value and continue (can't be used if p > n)
      - Mixed selection: start with null model, add variables according to best fit, if the p-value for some variable becomes large, remote it from the model
  - **How well does the model fit the data?**
    - Common metrics of model fit: RSE and $R^2$ (fraction of variance explained)
    - $R^2$ will always increase when more variables are added to the model
    - It can be useful to plot the data and look at the fit
    - It's possible that there are interactions between the predictors, which can be accomodated in the model
  - **What response value should we predict and how accurate is our prediction?**
    - Uncertainties associated with the prediction of Y from X and the model:
      1. The coefficient estimates $\hat{\beta}_1,\hat{\beta}_1,...$ are estimates for the true coefficients $\beta_0,\beta_1,...$, so the least squares plane (resulting model) is only an estimate of the true population regression plane. The inaccuracy is related to the reducible error. We can calculate a *confidence interval* to determine how close $\hat{Y}$ is to $f(X)$.
      2. Linear models are a very simple approximation, so there will be model bias (we are estimating a linear approximation to the true surface).
      3. Even if we knew the true values, there's always the irreducible error $\epsilon$. We can use *prediction intervals* to figure out how close $\hat{Y}$ is to $Y$ (will be wider than confidence intervals because they incorporate both the reducible and irreducible error).
- Other considerations for regression
  - Qualitative predictors - we can have labels or categories
    - These can be encoded in dummy variables (e.g. for gender, code 1 for female and 0 for male)
    - With more than two levels, we can have one-hot encoding (e.g. with four levels, [0 0 1 0] will correspond to variable of level 3) or dummy variables (one less than the number of levels where the first level is the baseline)
  - Extensions of the linear model
    - Linear regression makes pretty restrictive assumptions (the predictor and response relationship is additive and linear)
    - Additive assumption:
      - Perhaps there is a relationship between two predictors (an *interaction effect*), so instead of $Y = \beta_0+\beta_1X_1+\beta_2X_2+\epsilon$, we can have  $Y = \beta_0+\beta_1X_1+\beta_2X_2+\beta_3X_1X_2+\epsilon$
      - Hierarchical principle: if we include an interaction in the model, we must also include the main effects, even if they are not significant
    - Linear assumption:
      - We can extend the linear model to incorporate non-linear relationships using polynomial regression, e.g. $Y = \beta_0 + \beta_1X_1 + \beta_2X_1^2 + \epsilon$
      - This is still a linear model, but the fit will be non-linear
  - Potential problems:
    - **Non-linearity** - can be identified using a residual plot (plotting $y_i - \hat{y}$ versus $x_i$). If there is no pattern in the residuals, the fit is okay, if there is a pattern - there may be a problem.
    - **Correlation of error terms** - we assume that there is no correlation, but if there is, our estimated standard errors will underestimate the true error (so CI will be narrower). For example, if we accidentally doubled the data, the n is now technically 2n and our confidence intervals are off by $\sqrt2$. This can occur in time-series data, which we can check by plotting the residuals as a function of time. If adjacent residuals have similar values, we have a problem. Another example: if we have a study of weight vs height and accidentally include people from the same family or on the same diet. 
    - **Non-constant variance of error terms** - we assume that errors will have constant variance, but it may not be the case - e.g. if we look at income versus age, most teens will have a roughly similar income, but income for adults will vary widely. This will be noticeable as a funnel shape in a residual plot. We can fix this by fitting weighted least squares with weights proportional to inverse of the variance (so that weights for values with high variance will be lower).
    - **Outliers** - self-explanatory, values that are very far from the prediction. They can have a large effect on the fit. Residual plots can be used to identify outliers.
    - **High leverage observations** - values for which the predictor is outside of the nomal range. We can compute the leverage statistic for that: $h_i = \frac{1}{n}+\frac{(x_i-\bar{x})^2}{\sum_{i'=1}^n{(x'_i-\bar{x})^2}}$, so that h will increase as the distance from $x_i$ to $\bar{x}$ increases.
    - **Collinearity** - two or more predictors can be closely related to each other and will have very similar effects. The problem here is that the estimates will be very uncertain for these predictors. We can figure this out by looking at the correlation matrix of the predictors (but we can also have multicollinearity - collinearity between three or more variables).
  - Linear regression vs K-NN regression
    - Linear regression is parametric, $f(X)$ is linear, coefficients have simple interpretations and we can easily perform statistical testing. 
    - Non-parametric methods won't assume a form for $f(X)$ and might be more flexible, like K-NN regression.
    - K-NN regression: given K and a point $x_0$, identify the K observations closest to $x_0$ ($\mathcal{N}_0$). Then, average all the training responses and get $\hat{f}(x_0) = \frac{1}{K}\sum_{x_i\in\mathcal{N}_0}y_i$
    - Optimal K value will depend on the bias-variance trade off (small K - flexible fit, high variance, less smooth, large K - smoother fit, higher bias).
    - The parametric approach will be better if the parametric f is close to true f. So if data is approximately linear, linear regression will be better, if non-linear - non-parametric approaches are better.

## Chapter 4: Classification

- Classification - predicting a qualitative response
- Most widely used techniques: logistic regression, linear discriminant analysis, K-nearest neighbors
  - Other methods: generalized additive models, trees, random forests, support vector machines
- Linear regression is not appropriate for a qualitative response
  - Coding the variables will produce different linear models with a different set of predictions
  - For binary qualitative responses, we could use dummy variables (e.g. Y = 0 if outcome 1, Y = 1 if outcome 2). The $X\hat{\beta}$ obtained using linear regression will be an estimate of Pr(Outcome 2|X).
- **Logistic regression**
  - Instead of modeling the response Y directly, logistic regression models the probability that Y belongs to a category
  - Since linear regression will lead to non-sensical predictions (outside of [0 1]), we can use the logistic function, which has a sigmoid curve:
    - $p(X)=\frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}}$
    - $\frac{p(X)}{1-p(X)}=e^{\beta_0+\beta_1X}$ is the odds ([0 $\infty$])
    - $\log(\frac{p(X)}{1-p(X)})=\beta_0+\beta_1X$ is the log-odds or logit (linear in X)
  - Increasing X by one unit will change the log-odds by $\beta_1$ or multiply the odds by $e^{\beta_1}$
  - For estimating the coefficients, we could use non-linear least-squares, but *maximum likelihood* is the preferred method. 
  - The idea is that we estimate $\beta_0$ and $\beta_1$ such that, if we plug them into the logistic function, will give us a number close to 0 or 1 for the right category:
    - $\ell(\beta_0,\beta_1)=\prod_{i:y_1=1}p(x_i)\prod_{i':y'_1=0}(1-p(x_{i'}))$
    - We would like to maximize the likelihood function.
  - The outputs of logistic regression are similar to linear regression - we can compute standard errors, the z-statistic (same as t-statistic)
  - We can make predictions by plugging our estimated coefficients into the logistic function.
  - Special cases:
    - Multiple logistic regression
      - We can extend it to multiple predictors and generalize the equation as:
        - $$\log(\frac{p(X)}{1-p(X)})=\beta_0+\beta_1X+...+\beta_pX_p$$
      - Examining multiple predictors at once can be important, because the results for each predictor separately and together can be quite different
    - More than 2 response classes
      - We can also extend logistic regression to more response classes, but these extensions aren't used as often
- **Linear discriminant analysis**
  - In logistic regression, we model the conditional distribution of the response Y given X ($Pr(Y=k|X=x)$)
  - In linear discriminant analysis, we will model the distribution of the predictors X separately in each of the response classes and use Bayes' theorem to get $Pr(Y=k|X=x)$.
  - This method is useful for more stable parameter estimates and multi-class extensions
  - Bayes' theorem:
    - $P(Y|X)=\frac{P(X|Y)P(Y)}{P(X)}$
  - We want to classify an observation into one of K classes
  - Let $\pi_k$ represent the prior probability that an observation comes from class k
  - Let $f_k(x) \equiv Pr(X=x|Y=k)$ denote the probability density function for the predictor x given that our response is from class k ($f_k(x)$ will be large if there's a high probability that an observation in class k has X = x)
  - Substituting those into Bayes' theorem, we get: $Pr(Y=k|X=x)=\frac{f_k(x)\pi_k}{\sum_{l=1}^K\pi_lf_l(x)}$
  - Estimating $\pi_k$ is easy, because it will just be the fraction of the observations from class k
  - Estimating $f_k(X)$ will be harder, but we can assume some easy distribution like the normal distribution
  - $p_k(x)$ will be the posterior probability that an observation $X=x$ belongs to class k
  - *LDA with one predictor*:
    - Assume p=1 and that $f_k(x)$ is Gaussian:
      - $f_k(x)=\frac{1}{\sqrt{2\pi}\sigma_k}\exp({-\frac{1}{2\sigma^2_k}(x-\mu_k)^2})$
      - where $\mu_k$ and $\sigma^2_k$ are the mean and variance for class k
    - We can also assume that variance is the same across K classes
    - Now, we can plug the distribution into our linear discriminant analysis equation:
      - $p_k(x)=\frac{\pi_k\frac{1}{\sqrt{2\pi}\sigma}exp({-\frac{1}{2\sigma^2}(x-\mu_k)^2})}{\sum_{l=1}^K\pi_l\frac{1}{\sqrt{2\pi}\sigma}exp({-\frac{1}{2\sigma^2}(x-\mu_l)^2})}$
      - We can take the log of this equation and rearrange some stuff to get $\delta_k(x)=x\cdot\frac{\mu_k}{\sigma^2}-\frac{\mu_k^2}{2\sigma^2}+log(\pi_k)$, which will be largest for the right class for an observation (and is also a linear function of x, which is where the name comes from)
    - We still have to estimate the parameters $\mu_k$, $\pi_k$ and $\sigma^2$, which we approximate by:
      - $\hat{\mu}_k=\frac{1}{n_k}\sum_{i:y_i=k}x_i$ (average of all observations from class k)
      - $\hat{\sigma}^2=\frac{1}{n-K}\sum^K_{k=1}\sum_{i:y_i=k}(x_i-\hat{\mu}_k)^2$ (weighted average of the sample variances across the K classes)
      - $\pi_k=n_k/n$ (proportion of observations in class k)
  - In short, LDA assumes that the observations within each class k come from a normal distribution with a class-specific $\mu_k$ and $\sigma^2$, and then uses the Bayes classifier with these
  - *LDA with more than 1 predictor*:
    - We will assume that $X = (X_1, X_2...)$ is drawn from a multivariate Gaussian distribution, which will have a class specific $\mu_k$ and common $\Sigma$ (covariance matrix)
    - **Multivariate Gaussian distribution** assumes that each predictor also follows a Gaussian distribution and there is some correlation between the predictors (expressed in $\Sigma$). We write it as $X \sim N(\mu,\Sigma)$, where $\mu = E(X)$ (mean of p predictors) and $\Sigma = Cov(X)$ (p x p)
    - The full formula for the multivariate Gaussian is as follows:
      - $f_k(x)=\frac{1}{(2\pi)^{p/2}\Sigma^{1/2}}exp({-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)})$ (in the exponent, we now have -1/2 * 1 x p * inverted covariance, p x p * p x 1)
      - The delta for this case will be $\delta_k(x)=x^T\Sigma^{-1}\mu_k-\frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k+\log\pi_k$
      - (which is the vector version of the one predictor formula)
  - We can represent the predictions with a *confusion matrix*, with the true status and predicted status as columns and rows
  - We can also examine the sensitivity (true positive) and specificity (true negative)
  - A model can be adjusted by modifying the threshold value for the posterior probability (if we care more about true negatives vs true positives and vice versa)
  - A popular way of visualizing types of errors is the ROC curve (receiver operating characteristics): an ideal ROC curve will hug the top left corner (high true positive, low false positive rate)
  - *Quadratic discriminant analysis*
    - LDA assumes that our observations come from a Gaussian distribution (uni- or multivariate)
    - QDA assumes a separate covariance matrix for each class, so the delta is modified (each $\Sigma$ is now $\Sigma_k$ for class k)
    - This allows for more flexible boundaries, but also means we estimate a lot of extra parameters
- Comparison of classification methods
  - Logistic regression and LDA are closely connected to each other:
    - For 1 predictor and $p_1(x)$ and $p_2(x)=1-p_1(x)$, the log-odds in LDA is $\log(\frac{p_1(x)}{1-p_1(x)})=\log(\frac{p_1(x)}{p_2(x)})=c_0+c_1x$, where c are functions of the two means and the variance
    - In logistic regression, $log(\frac{p_1}{p_2})=\beta_0+\beta_1x$
    - So both are linear in x and produce linear decision boundaries
  - KNN has a different approach - it's non-parametric and makes no assumptions about the decision boundary
    - It will also not return any coefficients, so we wouldn't know which predictors are important

## Chapter 5: Resampling methods

- Drawing samples from a training set and refitting a model of interest on each sample - helps us get additional information
- Computationally expensive
- Common approaches: cross-validation and bootstrap
- **Cross-validation**
  - Test error is computed on new observations that may not be available
  - We can estimate the test error rate using available data by holding out a subset of the training observations and treating them as a test set
  - Validation set:
    - Randomly dividing data into a training and a validation set
    - Drawbacks: validation estimate can be highly variable, validation set error rate may overestimate the test error rate
  - Leave-one-out cross-validation:
    - This time, the validation set consists of a single variable
    - MSE is unbiased for the single estimate, but can also be highly variable
    - LOOCV estimate for test MSE: $CV_{(n)}=\frac{1}{n}\sum_{i=1}^{n}MSE_i$
    - Advantages: less bias, no test error rate overestimation
    - Disadvantages: can be computationally expensive
      - Shortcut for LS linear/polynomial regression: $CV_{(n)}=\frac{1}{n}\sum_{i=1}^{n}(\frac{y_i-\hat{y}_i}{1-h_i})^2$
      - where $h_i$ is the leverage (the amount that an observation influences its own fit)
  - K-fold cross-validation:
    - Randomly dividing the set of observations into k groups of approximately equal size (stratified)
    - $CV_{(k)}=\frac{1}{k}\sum_{i=1}^{k}MSE_i$
    - Typically involves k=5 or 10
    - Advantages: more feasible than LOOCV, bias-variance trade-off advantages
      - LOOCV has lower bias, but higher variance (because the output of LOOCV is highly correlated)
  - Often we are interested in finding the minimum point in the MSE curve and not the value - all CV methods are fairly good at identifying the correct CV when run on simulated data with known ground values
  - Cross-validation on classification:
    - We can use the number of misclassified observations in the same way as MSE.
- **Bootstrap**
  - Can be used to quantify the uncertainty associated with a given statistical method
  - Can be applied to many methods
  - The approach involves mimicking the process of getting new population samples, but from the same dataset
  - If we have two assets, X and Y, and we want to estimate the best values of $\alpha$ which will describe the proportion of X (or $1-\alpha$ for Y) that we can use (with minimum variance), we can estimate $\alpha$ with:
    - $\alpha = \frac{\sigma_Y^2-\sigma_{XY}}{\sigma_X^2+\sigma_Y^2-2\sigma_{XY}}$
  - Algorithm:
    - We perform sampling with replacement (same observation can occur more than once)
    - For a dataset $Z$, we can produce a boostrapped version $Z^{*1}$ and calculate an estimate of $\alpha$ on this dataset (and repeat for B bootstrapped datasets)
    - Then, we can calculate the standard error within these estimates:
      - $SE_{B}(\hat{\alpha})=\sqrt{\frac{1}{B-1}\sum_{r=1}^B(\hat{\alpha}^{*r}-\frac{1}{B}\sum_{r'=1}^B\hat{\alpha}^{*r'})^2}$

## Chapter 6: Linear model selection and regularisation

- Standard linear model equation $Y = \beta_0 + \beta_1X_1 + ... + \beta_pX_p+\epsilon$
- We can also extend the linear model framework or use non-linear methods, but the linear model is surprisingly useful and informative when compared to more complex methods 
- Therefore, we might want to improve linear models by using another fitting procedure for better accuracy and interpretability
  - Accuracy: if n >> p (we have much more samples than parameters), then the least squares estimates will be accurate for test data as well. For n > p, there can be a lot of variability that will result in a poor test fit. We can constrain the estimated coefficients to reduce the variance.
  - Interpretability: some variables that are used in a model may not be associated with a response, so including them will lead to unnecessary complexity. We can instead use feature/variable selection to remove irrelevant variables.
- Classes of methods: subset selection (finding a subset of p predictors), shrinkage/regularization (fitting a model involving all p predictors, but shrinking the coefficients towards zero), dimension reduction (projecting the p predictors into a M-dimensional subspace where M < p).
- **Subset selection**
  - Best subset selection:
    - $M_0$ - null model, just predicts the sample mean for each observation
    - For $k = 1..p$, fit all ${p \choose k}$ models with k predictors
    - Pick the best and call it $M_k$ (by RSS or $R^2$).
    - Select a single best model from the $M_0...M_p$ using CV prediction error, AIC, BIC or $R^2$
    - Caveat: RSS of the p+1 models will decrease monotonically (and $R^2$ will increase monotonically), so using training RSS will lead to selecting the model with the most variables
      - Therefore, using cross-validation or other adjusted approaches is needed
    - But the RSS/$R^2$ can also plateau at a certain k
    - We might want to use deviance instead of RSS (-2*maximized log likelihood) - the smaller, the better
    - This method is simple, but has computational limitations
  - Stepwise selection:
    - Subset selection can have issues with a large search space (large p)
    - Forward stepwise selection begins with no predictors and adds predictors one at a time, judging the additional improvement to the fit
    - Backward stepwise selection starts with a full model and removes least useful predictors
    - Hybrid approaches can combine adding most useful and removing least useful approaches
- We want to choose a model with the lowest test error
  - RSS and $R^2$ can be unsuitable for estimation of the test error
  - We can estimate the test error indirectly by adjusting the training error to incorporate the bias 
  - We can directly estimate the test error using CV/a test set
- Approaches for estimating model fit (for a model with **d** predictors)
  - $C_p$ estimate of the MSE: $\frac{1}{n}(RSS+2d\hat{\sigma}^2)$, where $\hat{\sigma}^2$ is the estimate of the variance of the error (this statistic adds a penalty of $2d\hat{\sigma}^2$ to the training RSS to adjust for the fact that the training error will be lower than test error
  - Akaike's information criterion (AIC) = $\frac{1}{n\sigma^2}(RSS+2d\hat{\sigma}^2)$
  - Bayesian information criterion (BIC) = $\frac{1}{n\sigma^2}(RSS+\log(n)d\hat{\sigma}^2)$ (the log places a larger penalty on too many variables)
  - These criteria will take on a small value for a low test error
  - The adjusted $R^2$ = $1-\frac{RSS/(n-d-1)}{TSS/(n-1)}$
    - Large value means a smaller test error
- Validation and cross-validation can be used as alternative approaches (will provide a direct estimate of the test error)
- **Shrinkage/regularization methods**
  - Instead of using a subset of predictors, we can shrink or regularize the coefficient estimates towards zero
    - Shrinking the coefficients can significantly reduce their variance
    - Techniques: ridge regression and lasso
  - Ridge regression
    - The least squares procedure estimates coefficients $\beta$ minimizing the RSS
      - $RSS = \sum_{i=1}^n(y_i-\beta_0-\sum_{j=1}^p\beta_jx_{ij})^2$
      - Ridge regression is similar, but it minimizes $RSS + \lambda\sum_{j=1}^p\beta_j^2$, where lambda is a tuning parameter
      - The term $\lambda\sum_{j=1}^p\beta_j^2$ is a shrinkage penalty that should shrink $\beta_j$ towards 0 and $\lambda$ controls how important that is for the overall fit
      - The shrinkage is applied to the coefficients $\beta_1..$, but not to the intercept!
      - $||\beta||_2$ denotes the $\ell_2$ norm of a vector ($\sqrt{\sum_{j=1}^p\beta_j^2}$), which measures how far beta is from 0
    - Least squares coefficient estimates do not depent on the scale, but ridge regression coefficients will change a lot
      - It might be useful to apply ridge regression after standardizing all the predictors to the same scale
    - Why does it work? Bias-variance trade-off. When using least squares estimates, we may have high variance, so a small change in the data will change the coefficients by a lot. Ridge regression takes care of the variance issue.
    - It's also computationally efficient
  - **Lasso**
    - Ridge regression will also use all p predictors, which might be a problem if you have a lot of predictors
    - Lasso has a similar formulation to ridge regression: it minimizes $RSS + \lambda\sum_{j=1}^p|\beta_j|$ (we use an $\ell_1$ penalty instead of an $\ell_2$, replacing $\beta^2$ with $|\beta|$)
    - Lasso can force some of the coefficient estimates to be equal to zero, performing variable selection
    - Lasse yields *sparse* models
  - Why does lasso select variables and ridge regression does not?
    - The constraint regions for the two $\ell$ norms will have different shapes and for lasso regression, that means the values can be set to zero
- Comparison
  - Lasso produces simpler and more interpretable models
  - The two have qualitatively similar behavior, but the lasso implicitly assumes that some of the coefficients are zero, while ridge regression does not, which means that the accuracy will depend on the data (although for real life datasets, we would not have ground truth for how many predictors are relevant) - best approach can be determined via CV
- Simple case:
  - n = p, **X** is a diagonal matrix with 1's on the diagonal
  - We are performing regression without an intercept, so we only need to find $\beta_1...\beta_p$ that minimize $\sum_{j=1}^p(y_i-\beta_j)^2$ and the least squares solution is $\hat{\beta}_j = y_j$
  - Ridge regression - minimize $\sum_{j=1}^p(y_i-\beta_j)^2 + \lambda\sum_{j=1}^p\beta_j^2$
  - Lasso - minimize $\sum_{j=1}^p(y_i-\beta_j)^2 + \lambda\sum_{j=1}^p|\beta_j|$
  - Ridge regression estimates $\hat{\beta}_j^R=\frac{y_i}{(1+\lambda)}$
  - Lasso estimates $\hat{\beta}_j^L = y_i-\lambda/2$ if $y_i > \lambda/2$
    -  $\hat{\beta}_j^L = y_i+\lambda/2$ if $y_i < -\lambda/2$
    - 0 if $|y_i| \leq \lambda/2$
- Bayesian interpretation:
  - We assume that the coefficient vector $\beta$ has some prior distribution $p(\beta)$ where $\beta = (\beta_0,\beta_1,...,\beta_p)^T$
  - The likelihood of the data is $f(Y|X,\beta)$
  - Posterior distribution $p(\beta|X,Y) \propto f(Y|X,\beta)p(\beta|X)=f(Y|X,\beta)p(\beta)$
    - Equality follows from assuming that X is fixed so $p(\beta|X)=p(\beta)$
  - Assume a linear model $Y = \beta_0+X_1\beta_1...+X_p\beta_p+\epsilon$
  - Assume also that $p(\beta)=\prod_{j=1}^pg(\beta_j)$, where g is a density function (so the probability of coefficients follows a function)
    - If g is Gaussian with zero mean and std = $f(\lambda)$, then posterior mode for $\beta$ (a.k.a. the most likely value) will be given by ridge regression
    - If g is a Laplace distribution, then the posterior mode is the lasso solution (the Laplace has a sharp peak at zero, which means that the prior expects some of the coefficients to be exactly at 0)
- Selecting $\lambda$
  - We can select it using CV, where we choose a grid of values and compute CV values for each
- **Dimension reduction methods**
  - All of the methods above have been defined using the original predictors $X_1,...,X_p$, but we can also transform the predictors and then fit a model to those
  - Assume that $Z_1,Z_2...Z_M$ represent M < p linear combinations of the p predictors
    - $Z_m = \sum_{j=1}^p\phi_{jm}X_j$, where $\phi_{1m}..$ are constants
  - We can fit a linear regression model:
    - $y_i=\theta_0+\sum_{m=1}^M\theta_mz_{im}+\epsilon_i$
  - Here, we reduce the problem of estimating p+1 coefficients to the problem of estimating M+1 coefficients where M < p
  - Steps: obtain the transformed predictors Z, then fit the model using those. Typical methods: principal components regression and partial least squares
  - Principal components regression
    - PCA is a tool for deriving a low-dimensional set of features from a large set of variables
    - The first principal component direction of the data will be the direction along which the observations vary the most (the line, projected onto which, the observations will be most spread out)
    - The second component will be orthogonal (perpendicular) to the first
    - We can have only as many components as dimensions in the data
    - PCR involves using the PCs in linear regression (the idea is that these components will be able to explain most of the variability and predict the relationship with the response - this is not guaranteed but is mostly useful anyway)
    - PCR will do best when the first couple principal components will be enough to capture most of the variability in the data
    - However, PCR is not a feature selection method, as we are still using a linear combination of all p predictors
  - Partial least squares
    - In PCR, the directions of most variability are designed in an unsupervised way (their relationship to Y has no bearing)
    - Therefore, in PCR, there no guarantee that the directions of most variability in the predictors will also be best for predicting Y
    - PLS is a supervised alternative to PCR (identifies features $Z_1...Z_M$, but takes into account both the relationship to original features and to the response)
    - In practice, often similar to ridge regression or PCR
- Considerations for higher dimensions
  - Most traditional techniques for regression and classification are intended for settings where n >> p
  - For p = n or p > n, least squares should not be performed - but if you do, you will get a fit (regardless of the actual relationship), and will most definitely overfit the data
  - Using approaches for adjusting the training set residuals or $R^2$ is not appropriate for high dimensional problems, since estimating the $\hat{\sigma}^2$ is problematic 
  - However, methods for reducing the models (selecting a subset of predictors, ridge regression, lasso and PCR) are suitable for regression in higher dimensions
    - Appropriate selection of the tuning parameters will be crucial
    - The test error will increase with the dimensionality of the problem, unless the additional features are really associated with the response (curse of dimensionality)
  - Interpreting results in high dimensions
    - We should be cautious with higher dimensions - multicollinearity can be a very important problem
    - One should never use the sum of squared errors, p-values, $R^2$ etc for high dimensional settings

## Chapter 7: Beyond linearity

- Linear models can be quite useful, but the linearity assumption is usually a pretty poor approximation
- We can improve the model by using regularization methods to reduce the complexity
- But we can also relax the linearity assumption while trying to maintain interpretability
- Methods:
  - polynomial regression (add extra predictors by raising original ones to a power)
  - step functions (cut the range into K regions, fit a piecewise constant function)
  - regression splines (extension of polynomial and step function regression, fitting a polynomial to each region k such that they join smoothly)
  - smoothing splines (similar to regression splines, but arise from minimizing a RSS criterion with a smoothness penalty)
  - local regression (similar to splines, but regions can smoothly overlap)
  - generalised additive models (extending all these methods to multiple predictors)
- **Polynomial regression**
  - We replace the standard linear model $y_i = \beta_0+\beta_1x_i+\epsilon_i$ with a polynomial: $y_i = \beta_0+\beta_1x_i+\beta_2x^2_i+\beta_3x^3_i+...+\beta_dx_i^d+\epsilon_i$
  - Coefficients can be estimated using the least squares method
  - It's unusual for d to be greater than 3 or 4
  - If we compute the fit at a particular point $x_0$, what is the variance of $f(x_0)$? We can use the estimates of coefficients $\hat{\beta}_j$ that we get from regression and the covariance between the coefficients to compute the estimated variance.
    - If $\hat{C}$ is the covariance between the coefficients $\hat{\beta}_j$ and $\ell_0^T=(1,x_0,x_0^2,x_0^3,x_0^4)$, then variance will be equal to $\ell_0^T\hat{C}\ell_0$
    - We can repeat this at every fitted point and plot the fitted curve
- **Step functions**
  - Using polynomials means we expect there to be a global structure in $f(x)$
  - We can also break up X into bins and fit a different function in each bin (similar to converting a continuous variable into an ordered vategorical variable)
  - We create cut-points $c_1..c_K$ within X and then make new variables such as $C_0(X)=I(X<c_1)$
    - $I$ is an indicator function that returns 1 when the condition is true
    - Also referred to as a dummy variable
  - We can now fit a linear model using $C_K(X)$ as predictors:
    - $y_i = \beta_0+\beta_1C_1(x_i)+\beta_2C_2(x_i)+...+\epsilon_i$ 
  - Usually these piecewise functions will miss some structure in the data - however, they are often used to define age bins in epidemiology or biostats
- **Basis functions**
  - The above two methods are special cases of the basis function approach
    - Basis functions $b(x)$ are transformations of X
    - For polynomial regression, $b_j(x_i)=x_i^j$
    - For piecewise regression, $b_j(x_i)=I(C_j \leq x < C_{j+1})$
  - With these methods, we can also use all of the available tools such as standard errors, F-stats etc
- **Regression splines**
  - Flexible class of basis functions
  - Created by specifying knots, making basis functions, using least squares to estimate coefficients
  - Piecewise polynomials:
    - Separate low degree polynomials over regions of X
    - Points where the coefficients change - knots
    - More knots - more flexibility
    - Just fitting curves may result in discontinuity between the regions, so we can constrain our fit to be continuous
    - Additional constraints: we can also constrain the first and second derivatives to be continuous to ensure more smoothness (these constraints will lower our degrees of freedom) - splines
  - We can use the basis model $y_i=\beta_0+\beta_1b_1(x_i)+\beta_2b_2(x_i)...$ to represent a regression spline, e.g. for a cubic:
    - $y_i=\beta_0+\beta_1b_1(x_i)+\beta_2b_2(x_i)+...+\beta_{K+3}b_{K+3}(x_i)+\epsilon_i$
  - Truncated power basis:
    - $h(x,\xi)=(x-\xi)^3$ if $x > \xi $
    - $h(x,\xi)=0$ otherwise
  - Choosing the number and locations of the knots:
    - More knots where we think the function will be unstable (depends on prior assumptions)
    - Uniform knots
  - Comparison to polynomial regression
    - Can give superior results - splines are flexible due to knots, polynomial regression is flexible in high degrees (can be unstable or overfit in certain regions)
- **Smoothing splines**
  - We want to have a function $g(x)$ that fits the observed data well (small RSS - $\sum(y_i-g(x_i))^2$)
  - Problem - we can have a function with really small RSS by just interpolating all of $y_i$ (really overfitting the data)
  - Constraint: smoothness
    - Minimize not just $\sum(y_i-g(x_i))^2$) (loss), but also $\lambda\int g''(t)^2dt$ (penalty)
    - $g(x)$ is the function, $g'(x)$ is the first derivative (change in g - slope at x), $g''(x)$ is the second derivative (change in slope at x) - the second derivative will measure how rough the function is
    - The integral is there to sum over the change in the function at t
    - $\lambda$ will control the effect of the penalty
  - Choosing $\lambda$
    - The tuning parameter will control the degrees of freedom
    - Degrees of freedom usually refer to the number of free parameters (e.g. number of coefficients)
    - A smoothing spline will have n parameters, but they will be very constrained, so it's better to look at effective degrees of freedom $df_{\lambda}$
    - Definition:
      - $\hat{g}_\lambda=S_\lambda y$, where $\hat{g}_\lambda$ is the solution to the minimization equation (loss + penalty), written as a matrix S times the response y
      - then, we define $df_\lambda=\sum_{i=1}^n \{ S_\lambda\}_{ii}$ (or the sum of the diagonal (i and i) entries of the matrix S)
    - We can choose $\lambda$ with cross-validation
- **Local regression**
  - Different approach - compute the fit at a point using only the nearby observations
    - Similar to nearest neighbors
  - We need to decide on a span s, which will control the flexibility of our model (s = k/n)
  - We assign a weight K to each point (based on distance from $x_0$)
  - Then, we fit a weighted least squares regression
- **Generalized additive models**
  - All of the above approaches can be seen as extensions of linear regression
  - Generalized additive models provide a framework for extending a linear model by allowing non-linear functions of each variable but having them all add up
  - GAMs for regression:
    - Multiple linear regression: $y_i=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+...+\beta_px_{ip}+\epsilon_i$
    - We can replace each linear component $\beta x$ with a smooth non-linear function $f_j(x_{ij})$, so $y_i=\beta_0+\sum_{j=1}^pf_j(x_{ij})+\epsilon_i$
  - GAMs for classification:
    - We can use the same $fj(x_{ij})$ approach in our logistic regression model:
      - $\log(\frac{p(X)}{1-p(X)})=\beta_0+\beta_1X+...+\beta_pX_p$
      - $\log(\frac{p(X)}{1-p(X)})=\beta_0+f_1(X_1)+...+f_p(X_p)$
  - Pros of GAMs:
    - Non-linear function f to each X - can model non-linear relationships
    - Could yield more accurate predictions
    - Since the model is additive, we can still look at the effect of individual predictors
  - Cons:
    - Model is restricted to be additive (but we can add interaction terms)

## Chapter 8: Tree-based methods

- Method
  - Tree-based methods involve segmenting the predictor space into regions
  - The mean/mode of the training observations in the region is used to make a prediction
  - Rules can be summarized in trees
- Approaches: decision trees, bagging, random forests and boosting (last three combine several trees, which improves accuracy)
- **Regression trees**
  - Example: predicting baseball player's salaries using regression trees (based on years played and number of hits)
  - The tree will consist of a series of splitting rules at points with most difference in the data (e.g. years < and > 4.5)
  - The regions within the tree are known as terminal nodes or leaves and the splitting points are known as internal nodes
  - Regression trees can oversimplify the relationship, but are easily interpretable
  - Process: divide the predictors space into J distinct and non-overlapping regions, for every observation in region $R_j$, the prediction will be a mean of all regions in $R_j$
  - The regions could have any shape, but it's easiest to divide the space into high-dimensional rectangles (boxes)
  - The goal is to find the boxes that minimize the RSS $\sum_{j=1}^{J}\sum_{i \in R_j}(y_i-\hat{y}_{R_j})^2$
  - Considering every possible partition is computationally intense, so we use a top-down (begins from the top and successively splits the space) greedy (best split is made at every step without look-aheads) approach - recursive binary splitting
  - We select the predictor $X_j$ and the cut-point $s$ such that the split reduces RSS the most
    - Next, we repeat the process, looking for the best predictor and cut-point
  - This process might over-fit the data by creating a tree that is too complex
    - We could build the tree only up until the point where the decrease in RSS is higher than a threshold
    - We can also prune our large tree to obtain a sub-tree
      - We can then test these sub-trees using CV
      - Cost complexity pruning - a way to select a small subset of trees for consideration (we examine a sequence of trees indexed by a non-negative tuning parameter $\alpha$)
- **Classification trees**
  - Similar to regression trees, but predict a qualitative response
    - Instead of using the mean, we attribute an observation to the most commonly occurring class within a region
    - Instead of RSS, we use the classification error rate
      - Fraction of observations that don't belong to the most common class ($E = 1-max_k(\hat{p}_{mk})$)
      - This method may not be the best, we can also use these:
        -  Gini index $G = \sum_{k=1}^K(\hat{p}_{mk})$ (total variance across K classes - small if a node contains observations from a single class)
        - Entropy $D = -\sum_{k=1}^K\hat{p}_{mk}\log\hat{p}_{mk}$ (near 0 if the node has mostly observations from one class)
    - Nodes can even have the same predicted value (e.g. there's a split, but both branches result in "No" classification) - this is done to increase node purity (separation)
- Trees vs linear models:
  - Linear regression: $f(X)=\beta_0+\sum_{j=1}^pX_j\beta_j$
  - Regression trees: $f(X) = \sum_{m=1}^Mc_m*1_{X\in R_m}$
    - Trees may outperform where there's a complex non-linear relationship between the features and the response
- Pros:
  - Easy to explain
  - May more closely mirror human decision-making
  - Easy to display
  - Handle qualitative predictors
- Cons:
  - Less predictive accuracy
  - Can be susceptible to small changes in the data
- **Bagging**
  - One of the methods to improve trees
  - Decision trees suffer from high variance (split the data into two parts, fit trees, possibly get different results)
  - Bagging is also known as bootstrap aggregation
  - Averaging a set of observations reduces variance (in a set of n observations with variance $\sigma^2$, averaging will lead to the mean having variance $\sigma^2/n$)
  - General approach: calculate our $\hat{f}$ using separate training sets and average them to get a low variance model 
    - Can be infeasible because we don't have access to multiple training sets
    - We can bootstrap - take repeated samples from the same data set
  - Bagging - construct B regression trees using B bootstrapped training sets, average the predictions
    - Can be extended to classification trees by using a majority class instead of the average
  - We will have remaining out of bag observations (ones that are not used to fit a given tree)
  - We can predict the response for the ith observation using each tree in which this observation was not used and average these predicted responses - then, we can use it to compute an error
  - How do we interpret resulting models? When we average over multiple trees, there is no longer one clear model and visualization
    - We can use RSS or the Gini index (mean decrease in) to get a summary of the importance of each predictor
- **Random forests**
  - Improves over bagging 
  - We build a number of decision trees on bootstrapped samples, but the splits only use m out of p predictors (typically, $m \approx \sqrt p$)
  - Rationale: there may be a very strong predictor in the dataset which will influence every bootstrapped tree - the predictions will be highly correlated and averaging them will not reduce the variance as much (because they are very similar)
  - If m = p, this is bagging
- **Boosting**
  - Can also improve predictions
  - In boosting, trees are grown sequentially - so the next tree uses information from the previous tree
    - We gradually add new decision trees into the fitted function, fitting them using residuals
    - Parameters - B (number of trees, should not be too large), $\lambda$ controls the learning rate and d (number of splits)
  - Often, smaller trees are enough in this case

## Chapter 9: Support Vector Machines

- Approach for classification, considered one of the best
- Generalization of the maximal margin classifier (assumes a linear boundary)
- Generally intended for 2 classes
- **Maximal margin classifier**
  - A hyperplane is a flat subspace of dimension p-1 in a p-dimensional space (analogous to a line in 2D space or a plane in 3D space)
  - In two dimensions, defined by $\beta_0+\beta_1X_1+\beta_2X_2=0$ (any $(X_1,X_2)^T$ for which the equation is = 0 is a point in the hyperplane)
  - The hyperplane divides our space into two halves (the side on which a point lies can be determined by the sign of the left-hand side)
  - Classification using a hyperplane:
    - We have an $n \times  p$ data matrix X (n observations in p-dimensional space) that fall into two classes
    - There can be many separating hyperplanes that separate the two classes
    - $\beta_0+\beta_1X_1+\beta_2X_2 > 0$ if $y_i=1$  and $\beta_0+\beta_1X_1+\beta_2X_2 < 0$ if $y_i=-1$
    - The magnitude of $f(x^*)$ will indicate how far this point is from the hyperplane
  - We need to have a way to decide on the best hyperplane, and a good choice is the maximal margin/optimal separating hyperplane (farthest from the training observations)
    - We compute the perpendicular distance from each training observation to a given hyperplane (this is known as the margin)
    - The observations that are closest to the hyperplane and define its borders are known as support vectors
  - Definition of the problem:
    - Maximize M (from $\beta_0$ to M) subject to $\sum_{j=1}^p\beta_j^2=1$, $y_i(\beta_0+\beta_1x_{i1}+...+\beta_px_{ip}) \geq M$ $\forall$ (for all) i = 1,..,n
    - The constraint $\geq M$ guarantees that observations will be on the right side and $\sum_{j-1}^p\beta_j^2=1$ does not constrain the hyperplane, but makes sure that the distance is kept (?)
  - Non-separable case
    - sometimes, there is no good way to separate classes with a hyperplane
    - we can define a hyperplane that *almost* separates the classes using a soft margin
    - generalization to the non-separable case is known as the support vector classifier
- **Support vector classifier**
  - Sometimes we can either not separate the observations or not separate them well enough (so that adding 1-2 observations will change the decision boundary a lot)
  - We may not want to separate observations perfectly, but instead ensure more robustness to individual observations and better classification of most of the training observations
  - SVC allows some of the observations be on the wrong side of the margin/hyperplane
  - It has the same definition as the maximal margin classifier, except that $y_i(\beta_0+x^T\beta)\geq M(1-\epsilon_i)$, where $\epsilon_i \geq 0$, $\sum_{i=1}^n\epsilon_i \leq C$ (there is a non-negative parameter (*slack variable*) for each observation and the sum of these parameters is less than C)
    - If $\epsilon_i=0$, then observation i is correctly classified, but if $\epsilon_i>0$, the observation is on the wrong side of the margin and the ith observation has violated the margin
    - C determines the amount of violations to the margin that we will tolerate
      - Generally chosen via CV (also controls the bias-variance trade-off, like many tuning parameters mentioned)
  - Observations that lie on the margin or on the wrong side of the margin are known as support vectors (because only they affect the SVC)
- **Support vector machines**
  - Non-linear decision boundaries - we have addressed them using polynomial functions of predictors in previous chapters
  - We could also fit a SVC using regular and polynomial predictors ($X_1,X_1^2...$)
    - However, this might make computations unmanageable
  - We can also extend the support vector classifier in a specific way using *kernels*
  - The solution to the SVC problem involves only the inner products of the observations
    - So if we have two vectors a and b, the inner product is $\langle a,b \rangle = \sum_{i=1}^ra_ib_i$
    - The linear SVC: $f(x)=\beta_0+\sum_{i=1}^n\alpha_i\langle x,x_i\rangle$, where alpha is a parameter that we choose per training observations by using the inner products
    - We could also replace the inner product with the generalization of the inner product of the form $K(x_i,x_{i'})$ which is called the kernel. The kernel quantifies the similarity  of two observations.
    - If we just use the sum of the product of the two observations, we will have a linear kernel
    - We can also have polynomial kernels, radial kernels etc
- More than two classes
  - The separating hyperplane doesn't really extend to >2 classes
  - Alternative approaches: one vs one and one vs all
  - *One-versus-one*: construct $K \choose 2$ SVMs (combinations of 2 out of K classes) SVMs - e.g. compares kth class (coded as +1) to k+1th class (coded as -1). Then tally the number of times the observation is assigned to each class and assign it to the most common out of the $K \choose 2$ classifications
  - *One-versus-all*: fit K SVMs, each time comparing one of the K classes to the K-1 classes. If $\beta_{0k},\beta_{1k}..$ are the parameters from comparing class k to the other, we will assign the observation to the class for which$\beta_{0k}+\beta_{1k}x^*_1..$ is the highest ($x^*$ being the test observation).
- SVMs and logistic regression
  - The idea of SVMs is different from other classification methods - especially because it allows violations of the boundary and uses kernels to expand the feature space 
  - We can rewrite the criterion for the SVC as minimizing $\beta_0,\beta_1..$ subject to $\{\sum_{i=1}^n\max[0,1-y_if(x_i)]+\lambda\sum_{j=1}^p\beta_j^2\}$, where lambda is a tuning parameter that defines how many violations to the margin are tolerated (the last term is the ridge penalty term)
  -  Now this will fit the loss+penalty form that we also saw for ridge regression and lasso
  - In this case, the loss is called *hinge loss*
    - In this loss function, it takes on the value of 0 for every observation that's on the correct side of the classifier
  - The kernel method is also not unique - we can use logistic regression or other methods with non-linear kernels
  - There is an extension of SVMs for regression - support vector regression, in which we minimize a loss function that only cares about the residuals larger than a threshold (similar to how the SVM cares about the values that are close/on/beyond the margin)

## Chapter 10: Unsupervised learning

- We use UL when we only have features and observations, but not associated response variables
- Tools: PCA (informative data visualization), clustering (discovering subgroups in the data)
- Unsupervised learning can be much more challenging, especially because the goal is subjective and there is no response variables to validate the algorithm on
- **Principal component analysis**
  - Principal components allow us to summarise a set of correlated variables with a smaller number of variables
  - If we have p = 10, examining 2D scatterplots of data is tedious - we need to reduce the dimensionality of the data
    - In PCA, each dimension is a linear combination of P features
  - The first principal component is the normalized ($\sum_{j=1}^p\phi_{j1}^2=1$) linear combination of the features: $Z_1=\phi_{11}X_1+\phi_{21}X_2..$
  - The $\phi$ are the loadings of the principal component (coefficients) and can be summarized in a vector. The sum of squares is normalized to one, because otherwise the variance might be too large.
  - We are maximizing $\frac{1}{n}\sum_{i=1}^n(\sum_{j=1}^p\phi_{j1}x_{ij})^2$ subject to the sum of squares = 1 constraint
    - We can also write this as maximizing $\frac{1}{n}\sum_{i=1}^nz^2_{ij}$ - we maximize the (linear combination of the data*coefficient)/n - sample variance of the n values of $z_{i1}$
  - The principal component directions $\phi$ are the ordered eigenvectors of $X^TX$ and the variances of the components are the eigenvalues - we can solve the problem through eigendecomposition
  - The second principal component has the same form, but we constrain it to be orthogonal (uncorrelated) to the first component
  - The PC vectors are the directions in feature space along which the data varies the most, the PC scores are projections along these directions
  - Another interpretation:
    - PCs provide low-dimensional linear surfaces that are closest to the observations
    - Single dimension of the data that lies as close as possible to all of the data points to provide a good summary of the data
    - $x_{ij}\approx\sum_{m=1}^Mz_{im}\phi_{jm}$ (taking the PC vectors and loadings gives us an approximation of the data)
  - More details:
    - Values should have a mean of zero
    - Scaling variables has an effect - should be normalized
    - Each loading vector is unique (may have the opposite sign)
    - We can calculate the proportion of variance explained (how much information is lost by projecting onto the first few components)
      - Total variance $\sum_{j=1}^pVar(X_j)=\sum_{j=1}^p\frac{1}{n}\sum_{i=1}^nx_{ij}^2$
      - Variance of the mth component $\frac{1}{n}\sum_{i=1}^n(\sum_{j=1}^p\phi_{jm}x_{ij})^2$ 
      - Divide the two by each other, get the variance explained
    - How to decide how many components to use? Depends! We can look at the plots and decide on the smallest amount that explains enough variance in the data
      - If we are doing a supervised analysis, we can use cross-validation
- **Clustering methods**
  - Methods that partition the dataset into distinct groups of similar points
  - Best known methods: K-means clustering (specified number of clusters) and hierarchical clustering (no prior number of clusters)
  - We can cluster observations based on the features or cluster features based on the observations
  - **K-means**:
    - Each observation will belong to one of the K clusters and the clusters are non-overlapping
    - $C_1..$ are the sets of indices of the observations in each cluster
    - A good cluster is one for which the within-cluster variation is as small as possible
      - minimize ${\sum_{k=1}^KW(C_k)}$
      - what is W? most commonly, squared Euclidian distance: $\frac{1}{|C_k|}\sum_{i,i'\in C_k}\sum_{j=1}^p(x_{ij}-x_{i'j})^2$ (we minimize the squared distance between each pair of observations in the cluster divided by the total number of observations in the cluster)
    - Harder than it seems - there are a lot of ways to partition n observations into K clusters
    - Algorithm:
      - Assign a number from 1 to K to each of the observations (initial cluster assignments)
      - Iterate until the cluster assignments are stable:
        - Compute the centroid (vector of the p feature means for the observations)
        - Assign each observation to the cluster whose centroid is closest to it
    - We can achieve the local optimum which is often good enough
      - But we should run the algorithm with different cluster assignments to avoid overly depending on initial values
  - **Hierarchical clustering**
    - Results in a tree-like representation of the clusters
    - The distance to the top of the tree and to the next level defines the similarity of the observations
      - The proximity along the horizontal axis does not define the similarity (can be reordered)
    - Algorithm:
      - Define a dissimilarity measure for pairs of observations (commonly - also Euclidian distance)
      - Start from the bottom of the tree, treat each observation as its own cluster
      - Fuse clusters that are most similar
      - Proceed up the tree
    - To define dissimilarity between clusters with multiple observations, we define *linkage* 
      - Complete - maximal dissimilarity - compute all pairwise metrics, choose largest
      - Single - minimal - compute all pairwise distances, choose smallest
      - Average - compute all, average
      - Centroid - dissimilarity between the mean vector of length p for each cluster
- Issues in clustering:
  - Should be observations be standardized?
  - How to choose dissimilarity measure?
  - How many clusters (for K-means)?
- Validating clusters - there are ways of assigning p-values to clusters, but this is a complicated issue
- Other considerations
  - Some values may truly not be in any cluster - we can use a *mixture model* (soft version of clustering)
  - Clustering methods are not usually robust to perturbations
  - 